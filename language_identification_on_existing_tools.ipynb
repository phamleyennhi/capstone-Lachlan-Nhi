{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09946e59",
   "metadata": {},
   "source": [
    "## Data Colletion from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d9e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a73971c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def date_range(start, end):\n",
    "    start_date = datetime.strptime(start, '%Y-%m-%d').date()\n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d').date()\n",
    "    delta = end_date - start_date \n",
    "    days = [start_date + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "    return list(map(lambda n: n.strftime(\"%Y-%m-%d\"), days))\n",
    "\n",
    "data_collection_period = date_range('2022-01-01', '2022-09-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7d28629",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"for\", \"on\", \"an\", \"a\", \"of\", \"and\", \"in\", \"the\", \"to\", \"from\"]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    if type(tweet) == np.float:\n",
    "        return \"\"\n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stopwords]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def get_most_recent_tweets(city, amount, distance = '20km'):\n",
    "    df_city = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(f'near:\"{city}\" within:{distance}').get_items(), amount))[['date', 'rawContent']]\n",
    "    df_city['cleaned_content'] = [clean_tweet(i) for i in df_city['rawContent']]\n",
    "    df_city.to_csv(f\"data/{city}/{amount}_most_recent_tweets.csv\")\n",
    "    return df_city\n",
    "\n",
    "\n",
    "def get_tweets_over_period(city, amount_per_day, data_collection_period, distance='20km'):\n",
    "    df_city = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(f'near:\"{city}\" within:{distance} since:{data_collection_period[0]} until:{data_collection_period[1]} ').get_items(), amount_per_day))[['date', 'rawContent']]\n",
    "    for i in range(2, len(data_collection_period)-1):\n",
    "        df_temp = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(f'near:\"{city}\" within:{distance} since:{data_collection_period[i]} until:{data_collection_period[i+1]} ').get_items(), amount_per_day))[['date', 'rawContent']]\n",
    "        df_city = df_city.append(df_temp)\n",
    "    df_city.to_csv(f\"data/{city}/{amount_per_day*len(data_collection_period)}_tweets_over_period.csv\")\n",
    "    return df_city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eae2257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported unified_card type on tweet 1501659063037403136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.7 s, sys: 2.28 s, total: 29 s\n",
      "Wall time: 23min 50s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 23:59:27+00:00</td>\n",
       "      <td>Ê¥ãÊúç3ÁùÄ„Å®„Çµ„É≥„ÉÄ„É´„Å®„Ç¢„Ç§„Ç∑„É£„Éâ„Ç¶„ÇíÂπ¥Êú´Ë≤∑„Å£„Å¶1Êó•„Å´‰∏ã„Çç„Åó„Åü„Çâ„ÄÅ„Å±„Çä„Å£„Å®„ÅäÊ≠£ÊúàÊ∞óÂàÜ„Å´üéç\\n„ÅÑ„Å§„Å∂„ÇäÔºü„Å®„ÅÑ„ÅÜ4ÈÄ£‰ºë„ÄÇ„Åê„Éº„Éº„Éº„Å£„Åô„ÇäÂØù„Å¶„ÄÅ„Çà„ÅÜ„ÇÑ„ÅèÂæ©Ê¥ª‚ú®</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 23:59:01+00:00</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 23:58:48+00:00</td>\n",
       "      <td>Just posted a photo @ Our Tampines Hub https://t.co/QjJN4MJnfB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 23:58:29+00:00</td>\n",
       "      <td>Just posted a photo @ Our Tampines Hub https://t.co/QUz8UuK2Sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 23:58:15+00:00</td>\n",
       "      <td>@ShibinMeta Eth okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2022-08-31 22:44:03+00:00</td>\n",
       "      <td>@Mohamed08334779 Masha allah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2022-08-31 22:42:29+00:00</td>\n",
       "      <td>NOPE https://t.co/G4YfsotYO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2022-08-31 22:41:02+00:00</td>\n",
       "      <td>@MattLegal13 @bod_republic Haha agak lama sudah üò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2022-08-31 22:40:59+00:00</td>\n",
       "      <td>Hold on tight to the ones you love. Life is fleeting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2022-08-31 22:40:22+00:00</td>\n",
       "      <td>Just posted a photo @ 121 Telok Ayer Street https://t.co/7b9dVNav61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24200 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2022-01-01 23:59:27+00:00   \n",
       "1  2022-01-01 23:59:01+00:00   \n",
       "2  2022-01-01 23:58:48+00:00   \n",
       "3  2022-01-01 23:58:29+00:00   \n",
       "4  2022-01-01 23:58:15+00:00   \n",
       "..                       ...   \n",
       "95 2022-08-31 22:44:03+00:00   \n",
       "96 2022-08-31 22:42:29+00:00   \n",
       "97 2022-08-31 22:41:02+00:00   \n",
       "98 2022-08-31 22:40:59+00:00   \n",
       "99 2022-08-31 22:40:22+00:00   \n",
       "\n",
       "                                                                   rawContent  \n",
       "0   Ê¥ãÊúç3ÁùÄ„Å®„Çµ„É≥„ÉÄ„É´„Å®„Ç¢„Ç§„Ç∑„É£„Éâ„Ç¶„ÇíÂπ¥Êú´Ë≤∑„Å£„Å¶1Êó•„Å´‰∏ã„Çç„Åó„Åü„Çâ„ÄÅ„Å±„Çä„Å£„Å®„ÅäÊ≠£ÊúàÊ∞óÂàÜ„Å´üéç\\n„ÅÑ„Å§„Å∂„ÇäÔºü„Å®„ÅÑ„ÅÜ4ÈÄ£‰ºë„ÄÇ„Åê„Éº„Éº„Éº„Å£„Åô„ÇäÂØù„Å¶„ÄÅ„Çà„ÅÜ„ÇÑ„ÅèÂæ©Ê¥ª‚ú®  \n",
       "1                                                                        100%  \n",
       "2              Just posted a photo @ Our Tampines Hub https://t.co/QjJN4MJnfB  \n",
       "3              Just posted a photo @ Our Tampines Hub https://t.co/QUz8UuK2Sv  \n",
       "4                                                        @ShibinMeta Eth okay  \n",
       "..                                                                        ...  \n",
       "95                                               @Mohamed08334779 Masha allah  \n",
       "96                                               NOPE https://t.co/G4YfsotYO0  \n",
       "97                          @MattLegal13 @bod_republic Haha agak lama sudah üò≠  \n",
       "98                      Hold on tight to the ones you love. Life is fleeting.  \n",
       "99        Just posted a photo @ 121 Telok Ayer Street https://t.co/7b9dVNav61  \n",
       "\n",
       "[24200 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_most_recent_tweets(\"Singapore\", 3000, distance='20km')\n",
    "get_tweets_over_period(\"Singapore\", 100, data_collection_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad558acc",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d1c1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "english_words_set = set(nltk.corpus.words.words())\n",
    "\n",
    "# # sanity test for some english words\n",
    "# list_english_words = list(english_words_set )\n",
    "# print(\"sustainability\" in list_english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9e1e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "995336ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_to_csv(file_name, list_words):\n",
    "    with open(f\"{file_name}.csv\", 'w', encoding='UTF8', newline='\\n') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"word\", \"frequency\"])\n",
    "        for tup in list_words:\n",
    "            writer.writerow(list(tup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2014726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_corpus(filename):\n",
    "    corpus = []\n",
    "    with open(filename) as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        next(csvreader)\n",
    "        for row in csvreader:\n",
    "            if len(row) > 3:\n",
    "                all_words_in_sentence = row[3].strip().split(\" \")\n",
    "                corpus += all_words_in_sentence\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def generate_word_frequency(city, approach, corpus):\n",
    "    counter=Counter(corpus)\n",
    "    most=counter.most_common()\n",
    "    write_to_csv(f\"data/{city}/{approach}_topMostFrequentWords\", most[:100])\n",
    "    write_to_csv(f\"data/{city}/{approach}_topLeastFrequentWords\", most[-100:-1])\n",
    "    \n",
    "    # non-English words\n",
    "    non_english_words = []\n",
    "    for tup in most:\n",
    "        if (lemmatizer.lemmatize(tup[0]) not in english_words_set and tup[0].isnumeric() == False):\n",
    "            non_english_words.append(tup)\n",
    "    write_to_csv(f\"data/{city}/{approach}_nonEnglishWords\", non_english_words)\n",
    "\n",
    "    \n",
    "def average_sentence_length(city):\n",
    "    total_length = 0\n",
    "    count = 0\n",
    "    with open(f\"data/{city}.csv\") as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        next(csvreader)\n",
    "        for row in csvreader:\n",
    "            if len(row) > 3:\n",
    "                all_words_in_sentence = row[3].split(\" \")\n",
    "                total_length += len(all_words_in_sentence)\n",
    "                count += 1\n",
    "    print(\"Average sentence length:\", total_length/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86b69fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3000 most recent tweets & analysis\n",
    "singlish_most_recent_tweets_corpus = generate_word_corpus(\"data/Singapore/GenerateMostRecentTweets/3000_most_recent_tweets.csv\")\n",
    "generate_word_frequency(\"Singapore\", \"GenerateMostRecentTweets/3000_most_recent_tweets\", singlish_most_recent_tweets_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07d3c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once, generate cleaned tweets separately for 2nd approach to reduce wait time\n",
    "df = pd.read_csv('data/Singapore/GenerateTweetsOverAPeriod/24400_tweets_over_period.csv', index_col=0)\n",
    "df['cleaned_content'] = [clean_tweet(i) for i in df['rawContent']]\n",
    "df.to_csv(f\"data/Singapore/GenerateTweetsOverAPeriod/24400_cleaned_tweets_over_period.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88d2d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate analysis for tweets over period 2022-01-01 and 2022-09-01\n",
    "singlish_over_period_tweets_corpus = generate_word_corpus(f\"data/Singapore/GenerateTweetsOverAPeriod/{100*len(data_collection_period)}_cleaned_tweets_over_period.csv\")\n",
    "generate_word_frequency(\"Singapore\", f\"GenerateTweetsOverAPeriod/{100*len(data_collection_period)}_cleaned_tweets_over_period\", singlish_over_period_tweets_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb6205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
